{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sundate (1, 1)\n",
      "+----------+\n",
      "|  run_date|\n",
      "+----------+\n",
      "|2024-xx-xx|\n",
      "+----------+\n",
      "\n",
      "OH_DATE: '2024-xx-xx'\n"
     ]
    }
   ],
   "source": [
    "# LIB IMP\n",
    "import sys \n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "#technical\n",
    "def query_to_variable (query):\n",
    "  query_output = spark.sql(query)  \n",
    "  query_output_list = query_output.collect()\n",
    "  query_variable = query_output_list[0][0]\n",
    "  return query_variable\n",
    "\n",
    "#exec for later\n",
    "list_format = \"\"\"\n",
    "#       LIST DICTIONARY CREATION       #\n",
    "OBL = {}\n",
    "for key, value in OB_Limit_List.items():\n",
    "    OBL['{}'.format(key)] = ''.join(\n",
    "        [\"'\" + i + \"',\" if i != value[-1] else \"'\" + i + \"'\" for i in value])\n",
    "OBB = {}\n",
    "for key, value in OB_Block_List.items():\n",
    "    OBB['{}'.format(key)] = ''.join(\n",
    "        [\"'\" + i + \"',\" if i != value[-1] else \"'\" + i + \"'\" for i in value])\n",
    "one_blk = {}\n",
    "for key, value in One_WOS_block_lists.items():\n",
    "    one_blk['{}'.format(key)] = ''.join(\n",
    "        [\"'\" + i + \"',\" if i != value[-1] else \"'\" + i + \"'\" for i in value])\n",
    "two_blk = {}\n",
    "for key, value in Two_WOS_block_lists.items():\n",
    "    two_blk['{}'.format(key)] = ''.join(\n",
    "        [\"'\" + i + \"',\" if i != value[-1] else \"'\" + i + \"'\" for i in value])\n",
    "three_blk = {}\n",
    "for key, value in Three_WOS_block_lists.items():\n",
    "    three_blk['{}'.format(key)] = ''.join(\n",
    "        [\"'\" + i + \"',\" if i != value[-1] else \"'\" + i + \"'\" for i in value])\n",
    "four_blk = {}\n",
    "for key, value in Four_WOS_block_lists.items():\n",
    "    four_blk['{}'.format(key)] = ''.join(\n",
    "        [\"'\" + i + \"',\" if i != value[-1] else \"'\" + i + \"'\" for i in value])\n",
    "one_cap = {}\n",
    "for key, value in One_WOS_Cap_Lists.items():\n",
    "    one_cap['{}'.format(key)] = ''.join(\n",
    "        [\"'\" + i + \"',\" if i != value[-1] else \"'\" + i + \"'\" for i in value])\n",
    "three_cap = {}\n",
    "for key, value in Three_WOS_Cap_Lists.items():\n",
    "    three_cap['{}'.format(key)] = ''.join(\n",
    "        [\"'\" + i + \"',\" if i != value[-1] else \"'\" + i + \"'\" for i in value])\n",
    "four_cap = {}\n",
    "for key, value in Four_WOS_Cap_Lists.items():\n",
    "    four_cap['{}'.format(key)] = ''.join(\n",
    "        [\"'\" + i + \"',\" if i != value[-1] else \"'\" + i + \"'\" for i in value])\n",
    "#       OUTPUT REVIEW       #\n",
    "print('#    1 BLOCK    #')\n",
    "print('one_blk')\n",
    "for key, value in one_blk.items():\n",
    "    print('{}: {}'.format(key, value))\n",
    "print('#    2 BLOCK    #')\n",
    "print('two_blk')\n",
    "for key, value in two_blk.items():\n",
    "    print('{}: {}'.format(key, value))\n",
    "print('#    3 BLOCK    #')\n",
    "print('three_blk')\n",
    "for key, value in three_blk.items():\n",
    "    print('{}: {}'.format(key, value))\n",
    "print('#    4 BLOCK    #')\n",
    "print('four_blk')\n",
    "for key, value in four_blk.items():\n",
    "    print('{}: {}'.format(key, value))\n",
    "print('#    1 WOS LIMIT    #')\n",
    "print('one_cap')\n",
    "for key, value in one_cap.items():\n",
    "    print('{}: {}'.format(key, value))\n",
    "print('#    3 WOS LIMIT    #')\n",
    "print('three_cap')\n",
    "for key, value in three_cap.items():\n",
    "    print('{}: {}'.format(key, value))\n",
    "print('#    4 WOS LIMIT    #')\n",
    "print('four_cap')\n",
    "for key, value in four_cap.items():\n",
    "    print('{}: {}'.format(key, value))\n",
    "print('#    OB Day Limit     #')\n",
    "for key, value in OBL.items():\n",
    "    print('{}: {}'.format(key, value))\n",
    "print('#    OB DAYS BLOCK   #')\n",
    "for key, value in OBB.items():\n",
    "    print('{}: {}'.format(key, value))\n",
    "\"\"\"\n",
    "sun_date= \"\"\"\n",
    "  with \n",
    "    yr as \n",
    "      (select distinct YEAR_NUM from schema.dim_dates where DAY_DESC=current_date())\n",
    "  select distinct\n",
    "    dt.DAY_DATE as run_date\n",
    "  from schema.dim_dates dt\n",
    "  join (select distinct\n",
    "          dt_B.MNTH_NAME\n",
    "          ,dt_A.DT_WK_END\n",
    "        from schema.dim_dates dt_A\n",
    "        join schema.dim_dates dt_B\n",
    "          on dt_A.DT_WK_END=dt_B.Day_Date\n",
    "        where dt_A.YEAR_NUM = (select * from yr)\n",
    "          and dt_A.day_date = current_Date()\n",
    "        order by dt_A.DT_WK_END,dt_B.MNTH_NAME) mth\n",
    "    on dt.DT_WK_END = mth.DT_WK_END\n",
    "  where dt.YEAR_NUM = (select * from yr)\n",
    "    and dt.day = 'SUN' \n",
    "  order by run_date\"\"\"\n",
    "view_name = 'sun_date'\n",
    "DATA = spark.sql(sun_date)\n",
    "DATA.createOrReplaceTempView(view_name)\n",
    "print(view_name,(DATA.count(), len(DATA.columns)))\n",
    "DATA.show()\n",
    "\n",
    "OH_DATE = spark.sql(\"SELECT run_date FROM sun_date\").collect()[0][0].strftime(\"'%Y-%m-%d'\")\n",
    "print('OH_DT:', OH_DT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#    1 BLOCK    #\n",
      "one_blk\n",
      "STR: ''''\n",
      "RGN: ''\n",
      "TLI: ''\n",
      "ITEMDIM2: ''\n",
      "DEPT: ''\n",
      "#    2 BLOCK    #\n",
      "two_blk\n",
      "STR: ''''\n",
      "RGN: ''\n",
      "TLI: ''\n",
      "ITEMDIM2: ''\n",
      "DEPT: ''\n",
      "#    3 BLOCK    #\n",
      "three_blk\n",
      "STR: ''''\n",
      "RGN: ''\n",
      "TLI: ''\n",
      "ITEMDIM2: ''\n",
      "DEPT: ''\n",
      "#    4 BLOCK    #\n",
      "four_blk\n",
      "4_blk\n",
      "STR: ''''\n",
      "RGN: ''\n",
      "TLI: ''\n",
      "ITEMDIM2: ''\n",
      "DEPT: ''\n",
      "#    1 WOS LIMIT    #\n",
      "one_cap\n",
      "STR: ''''\n",
      "RGN: ''\n",
      "TLI: ''\n",
      "ITEMDIM2: ''\n",
      "DEPT: ''\n",
      "#    3 WOS LIMIT    #\n",
      "three_cap\n",
      "STR: ''''\n",
      "RGN: ''\n",
      "TLI: ''\n",
      "ITEMDIM2: ''\n",
      "DEPT: ''\n",
      "#    4 WOS LIMIT    #\n",
      "four_cap\n",
      "STR: ''\n",
      "RGN: ''\n",
      "TLI: '1111123456','1111134567'\n",
      "ITEMDIM2: '660018','660019','660020'\n",
      "DEPT: '33','66','99'\n",
      "#    OB Day Limit     #\n",
      "1WOS: ''\n",
      "3WOS: ''\n",
      "4WOS: 'Y'\n",
      "#    OB DAYS BLOCK   #\n",
      "1WOS: ''\n",
      "2WOS: ''\n",
      "3WOS: ''\n",
      "4WOS: ''\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# LISTS FOR BLOCKS #\n",
    "One_WOS_block_lists = {\n",
    "  'STR': ['', ''],\n",
    "  'RGN': [''],\n",
    "  'TLI': [''],\n",
    "  'ITEMDIM2': [''],\n",
    "  'DEPT': ['']}\n",
    "Two_WOS_block_lists = {\n",
    "  'STR': [''],\n",
    "  'RGN': [''],\n",
    "  'TLI': [''],\n",
    "  'ITEMDIM2': [''],\n",
    "  'DEPT': ['']\n",
    "}\n",
    "Three_WOS_block_lists = {\n",
    "  'STR': [''],\n",
    "  'RGN': [''],\n",
    "  'TLI': [''],\n",
    "  'ITEMDIM2': [''],\n",
    "  'DEPT': ['']}\n",
    "Four_WOS_block_lists = {\n",
    "  'STR': [''],\n",
    "  'RGN': [''],\n",
    "  'TLI': [''],\n",
    "  'ITEMDIM2': [''],\n",
    "  'DEPT': ['']\n",
    "}\n",
    "# CAP LISTS #\n",
    "One_WOS_Cap_Lists = {\n",
    "  'STR': [''],\n",
    "  'RGN': [''],\n",
    "  'TLI': [''],\n",
    "  'ITEMDIM2': [''],\n",
    "  'DEPT': ['']}\n",
    "Three_WOS_Cap_Lists = {\n",
    "  'STR': [''],\n",
    "  'RGN': [''],\n",
    "  'TLI': [''],\n",
    "  'ITEMDIM2': [''],\n",
    "  'DEPT': ['']}\n",
    "Four_WOS_Cap_Lists = {\n",
    "  'STR': [''],\n",
    "  'RGN': [''],\n",
    "  'TLI': ['1111123456','1111134567'],\n",
    "  'ITEMDIM2': ['660018','660019','660020'],\n",
    "  'DEPT': ['33','66','99']}\n",
    "# Owned Brand Pref #\n",
    "OB_Limit_List = { \n",
    "  '1WOS' : [''],\n",
    "  '3WOS' : [''],\n",
    "  '4WOS' : ['Y']}\n",
    "OB_Block_List = {   \n",
    "  '1WOS' : [''],\n",
    "  '2WOS' : [''],\n",
    "  '3WOS' : [''],\n",
    "  '4WOS' : ['']}\n",
    "exec(list_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spine, Vertebra, and Decision Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPINE_QUERY = \"\"\"\n",
    "with \n",
    "  date_per as\n",
    "    (--for when primary spine data does not finish in time\n",
    "      select min(per_cd) as per_cd \n",
    "    from\n",
    "      (--per_cd for proper sun_date run\n",
    "        select distinct per_cd from schema.period_dim where TODATE = {0}\n",
    "        union\n",
    "      --per cd for max in store date (+1 to simulate how the sun_date works vs the expected saturdate for sales)\n",
    "        select max(per_cd)+1 from schema.sub_incl\n",
    "      )\n",
    "    )\n",
    "select distinct\n",
    "  spine.STORE_NBR,\n",
    "  TLI.top_level_item,\n",
    "  OM.WHSE_COST,\n",
    "  TLI.OB_IND,\n",
    "  TLI.ITEM_DEPT,\n",
    "  TLI.ITEMDIM2,\n",
    "  src.source as whse_nbr,\n",
    "  case when src.vnd_dlv_flg = 'Y' or substr(src.source,1,4) <> '5555' \n",
    "    then vend_mult\n",
    "  else greatest(coalesce(om.ord_mult,1),1)\n",
    "  end as ord_mult,\n",
    "  --spine.sub_days,\n",
    "  {0} as todate\n",
    "--primary item||store list. Top Level Item is encoded as a separate product code that must be joined to top level dim table\n",
    "from \n",
    "  (select distinct \n",
    "    prod_cd,\n",
    "    STORE_NBR,\n",
    "    min(min_per) as min_per,\n",
    "    max(max_per) as max_per,\n",
    "    max(max_per)-min(min_per) as sub_days\n",
    "  from\n",
    "    (select distinct\n",
    "      prod_cd,\n",
    "      STORE_NBR,\n",
    "      min(per_cd) as min_per,\n",
    "      max(per_cd) as max_per\n",
    "      from schema.sub_incl\n",
    "      where per_cd >= (select per_cd-29 from dt_per)\n",
    "        and per_cd < (select per_cd from dt_per)\n",
    "      group by\n",
    "      prod_cd,\n",
    "      STORE_NBR\n",
    "    union\n",
    "    select distinct\n",
    "      prod_cd,\n",
    "      STORE_NBR,\n",
    "      min(per_cd) as min_per,\n",
    "      max(per_cd) as max_per\n",
    "      from schema.sub_excl\n",
    "      where per_cd >= (select per_cd-29 from dt_per)\n",
    "        and per_cd < (select per_cd from dt_per)\n",
    "      group by\n",
    "      prod_cd,\n",
    "      STORE_NBR)\n",
    "  group by\n",
    "    prod_cd,\n",
    "    STORE_NBR\n",
    "  having max(max_per)-min(min_per) >= 4*7\n",
    "  ) spine\n",
    "--TOP LEVEL ITEM DIM RELATION\n",
    "join schema.dim_top_level_item TLI\n",
    "  ON spine.prod_cd = TLI.prod_cd\n",
    "--SOURCING INFORMATION\n",
    "join \n",
    "  (select distinct\n",
    "    a.item,\n",
    "    a.str_nbr,\n",
    "    b.source,\n",
    "    a.vend_multqty as vend_mult\n",
    "  from schema.sku_plan_dim a\n",
    "  join schema.sourcing b\n",
    "    on a.item = b.item\n",
    "    and a.str = b.dest\n",
    "    and a.run_date = b.run_date\n",
    "  where a.run_date = {0}\n",
    "    and date(b.eff_date) <= {0}\n",
    "    and date(b.disc_date) > {0}) src\n",
    "  on TLI.top_level_item = src.item\n",
    "  and spine.STORE_NBR = src.str_nbr\n",
    "--PREBUILT TOP LEVEL ITEM WAREHOUSE DATA\n",
    "left join work_schema.TLI_DC_DATA om\n",
    "  on spine.top_level_item = om.top_level_item\n",
    "  and src.source = om.whse_nbr\n",
    "\"\"\"\n",
    "DATA = spark.sql(SPINE_QUERY.format(OH_DT))\n",
    "view_name = 'Spine'\n",
    "DATA.createOrReplaceTempView(view_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Budget Data\n",
    "VERT1_QUERY = \"\"\"\n",
    "select distinct\n",
    "  spine.STORE_NBR,\n",
    "  spine.top_level_item,\n",
    "  spine.OB_IND,\n",
    "  spine.whse_nbr,\n",
    "  spine.whse_cost,\n",
    "  spine.ord_mult,\n",
    "  --bdgt min/max increase on 4_wk cap ITEM_DIMS for extra room before blocking and also for OB items\n",
    "  case \n",
    "    when OB_IND = 'Y'\n",
    "    or top_level_item in ({0})\n",
    "    or TLI.ITEM_DEPT in ({1})\n",
    "    or spine.ITEMDIM2 in ({2})\n",
    "      then greatest(least(coalesce(plan_dlrs/planned_csu,0)*7\n",
    "                          ,70)\n",
    "                    ,35)\n",
    "    else coalesce(budget_days,28) \n",
    "  end as budget_dos,\n",
    "  spine.todate\n",
    "from spine\n",
    "left join\n",
    "  (select distinct\n",
    "    bdgtfct.period_cd,\n",
    "    date.day_date,\n",
    "    dept.ITEM_DEPT,\n",
    "    bdgtfct.prod_cd,\n",
    "    bdgtfct.plan_dlrs,\n",
    "    bdgtfct.PLANNED_CSU,\n",
    "    ------- minimum 14 maximum 28 general budgets -------\n",
    "    greatest(least(coalesce(bdgtfct.plandlrs,1000)/coalesce(bdgtfct.planned_Csu,10)*7\n",
    "                  ,28)\n",
    "            ,14) as budget_days\n",
    "  from schema.budget bdgtfct\n",
    "  join schema.dim_dates date\n",
    "    on bdgtfct.period_cd = date.period_cd\n",
    "  join schema.dept_dim dept\n",
    "    on bdgtfct.prd_cd = dept.prd_cd\n",
    "  -- date filter to speed up query\n",
    "  where date.day_date >= current_date()-14\n",
    "  ) bdgt\n",
    "on spine.todate=bdgtfct.day_date\n",
    "  and spine.ITEM_DEPT=bdgtfct.ITEM_DEPT\n",
    "  \"\"\"\n",
    "view_name = 'Spine_Vert1'\n",
    "DATA = spark.sql(SPINE_QUERY.format(four_cap['TLI'],\n",
    "                                    four_cap['ITEMDIM2'],\n",
    "                                    four_cap['DEPT']))\n",
    "DATA.createOrReplaceTempView(view_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cost of Sales Units \n",
    "CSU_QUERY = \"\"\"\n",
    "  with \n",
    "    dt_per as\n",
    "      (select WK_per_cd from schema.dim_dates \n",
    "        where DAY_DATE = {0})\n",
    "  select distinct \n",
    "      CSU.top_level_item,\n",
    "      CSU.STORE_NBR,\n",
    "      --selecting the correct weekly sum depending on strength of recent weeks compared to least proximal 7\n",
    "      CASE\n",
    "        WHEN  CSU.G_CSU = CSU.mid_rcnt_CSU\n",
    "          then 6W_CSU\n",
    "        WHEN  CSU.G_CSU = CSU.nt_rcnt_CSU\n",
    "          then 13W_CSU\n",
    "        ELSE G_CSU\n",
    "      END as CSU,\n",
    "      CSU.G_CSU,\n",
    "      CASE\n",
    "        WHEN  CSU.G_CSU = CSU.mid_rcnt_CSU\n",
    "          then 6W_units\n",
    "        WHEN  CSU.G_CSU = CSU.nt_rcnt_CSU\n",
    "          then 13W_units\n",
    "        ELSE G_UNITS\n",
    "      END as SLS_UNITS,\n",
    "      CASE\n",
    "        WHEN CSU.G_CSU = CSU.mst_rcnt_CSU \n",
    "          then least ( 21, PER_DAYS)\n",
    "        WHEN  CSU.G_CSU = CSU.mid_rcnt_CSU\n",
    "          then least( 42, PER_DAYS)\n",
    "        WHEN  CSU.G_CSU = CSU.nt_rcnt_CSU\n",
    "          --13 wk period means max(91) per_days\n",
    "          then PER_DAYS\n",
    "        else 91\n",
    "      END as FLEXDAYS\n",
    "      ,dt.wk_end_dt \n",
    "    from \n",
    "      (SELECT distinct\n",
    "        csu_base.per_cd\n",
    "        ,top_level_item\n",
    "        ,STORE_NBR\n",
    "        , SUM(SLS_UNITS) OVER (PARTITION BY top_level_item, STORE_NBR ORDER BY csu_base.per_cd ROWS BETWEEN 12 preceding AND CURRENT ROW ) as 13W_units\n",
    "        , SUM(SLS_UNITS) OVER (PARTITION BY top_level_item, STORE_NBR ORDER BY csu_base.per_cd ROWS BETWEEN 5 preceding AND CURRENT ROW ) as 6W_units\n",
    "        , greatest(SUM(SLS_UNITS) OVER (PARTITION BY top_level_item, STORE_NBR ORDER BY csu_base.per_cd ROWS BETWEEN 12 preceding AND 6 preceding )\n",
    "                  ,SUM(SLS_UNITS) OVER (PARTITION BY top_level_item, STORE_NBR ORDER BY csu_base.per_cd ROWS BETWEEN 5 preceding AND 3 preceding )\n",
    "                  ,SUM(SLS_UNITS) OVER (PARTITION BY top_level_item, STORE_NBR ORDER BY csu_base.per_cd ROWS BETWEEN 2 preceding AND CURRENT ROW )\n",
    "                  ) G_UNITS\n",
    "        -- actual csu sums to be used for days of supply calculation.\n",
    "        --Note that we only need to create the 3 wk if it is in fact the greatest because we else into 3wk G_UNITS/CSU given the logic above. \n",
    "        , SUM(CSU_DLRS) OVER (PARTITION BY top_level_item, STORE_NBR ORDER BY csu_base.per_cd ROWS BETWEEN 12 preceding AND CURRENT ROW ) as 13W_CSU\n",
    "        , SUM(CSU_DLRS) OVER (PARTITION BY top_level_item, STORE_NBR ORDER BY csu_base.per_cd ROWS BETWEEN 5 preceding AND CURRENT ROW) as 6W_CSU\n",
    "        , greatest(SUM(CSU_DLRS) OVER (PARTITION BY top_level_item, STORE_NBR ORDER BY csu_base.per_cd ROWS BETWEEN 12 preceding AND 6 preceding)\n",
    "                  ,SUM(CSU_DLRS) OVER (PARTITION BY top_level_item, STORE_NBR ORDER BY csu_base.per_cd ROWS BETWEEN 5 preceding AND 3 preceding)\n",
    "                  ,SUM(CSU_DLRS) OVER (PARTITION BY top_level_item, STORE_NBR ORDER BY csu_base.per_cd ROWS BETWEEN 2 preceding AND CURRENT ROW )\n",
    "                  ) G_CSU\n",
    "        -- 3, 6, 13 wk buckets as well as their greatest (G_UNITS/CSU) created here.\n",
    "        , SUM(CSU_DLRS) OVER (PARTITION BY top_level_item, STORE_NBR ORDER BY csu_base.per_cd ROWS BETWEEN 2 preceding AND CURRENT ROW )\n",
    "          as mst_rcnt_CSU\n",
    "        , SUM(CSU_DLRS) OVER (PARTITION BY top_level_item, STORE_NBR ORDER BY csu_base.per_cd ROWS BETWEEN 5 preceding AND 3 preceding )\n",
    "          as mid_rcnt_CSU\n",
    "        , SUM(CSU_DLRS) OVER (PARTITION BY top_level_item, STORE_NBR ORDER BY csu_base.per_cd ROWS BETWEEN 12 preceding AND 6 preceding)\n",
    "          as nt_rcnt_CSU\n",
    "        --  week count for floating dos  --\n",
    "        , COUNT(per_cd) OVER (PARTITION BY top_level_item, STORE_NBR ORDER BY csu_base.per_cd ROWS BETWEEN 12 preceding AND CURRENT ROW ) * 7\n",
    "          as PER_DAYS\n",
    "      FROM\n",
    "        (SELECT distinct\n",
    "          FCT.per_cd\n",
    "          ,UPC.top_level_item\n",
    "          ,FCT.STORE_NBR\n",
    "          ,sum(FCT.SLS_UNITS) as SLS_UNITS\n",
    "          ,SUM(FCT.SLS_COST_DLRS) AS CSU_DLRS\n",
    "        FROM schema.SALES_UPC FCT\n",
    "        LEFT JOIN schema.DIM_UPC UPC\n",
    "          ON FCT.PROD_ID = UPC.PROD_ID\n",
    "        Where FCT.per_cd >= (select WK_per_cd-14 from dt_per)\n",
    "        GROUP BY\n",
    "          FCT.per_cd,\n",
    "          UPC.top_level_item,\n",
    "          FCT.STORE_NBR) csu_base\n",
    "        )CSU\n",
    "    Join schema.dim_dates dt\n",
    "      on CSU.per_cd = dt.WK_per_cd\n",
    "    where dt.WK_per_cd = (select WK_per_cd-1 from dt_per)\n",
    "    order by\n",
    "      top_level_item desc,\n",
    "      STORE_NBR\n",
    "\"\"\"\n",
    "view_name = 'CSU'\n",
    "DATA = spark.sql(CSU_QUERY.format(SDATE))\n",
    "DATA.createOrReplaceTempView(view_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision data joined to spine and base data calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DATA (123456789, 25)\n"
     ]
    }
   ],
   "source": [
    "BASE_QUERY = \"\"\"\n",
    "Select distinct \n",
    "  spine.todate,\n",
    "  spine.top_level_item,\n",
    "  spine.STORE_NBR,\n",
    "  spine.sub_days,\n",
    "  spine.ITEMDIM4,\n",
    "  spine.budget_dos,\n",
    "  spine.ord_mult,\n",
    "  oh.OH_DLRS,\n",
    "  csu.CSU,\n",
    "  arrv.arriv_units,\n",
    "  coalesce(spine.whse_cost,0)*coalesce(arrv.arriv_units,0) as 13wk_arriv_cost,\n",
    "  greatest(ROUND(COALESCE(oh.OH_DLRS,0)*FLEXDAYS / COALESCE(csu.CSU,0),2),0) AS STR_DOS,\n",
    "  greatest(round(coalesce(oh.OH_DLRS,0)*91 / (coalesce(spine.whse_cost,0)*coalesce(arrv.arriv_units,0)),2),0) as ftr_dos,\n",
    "  oh.OH_UNITS,\n",
    "  csu.SLS_UNITS,\n",
    "  -- weekly cap scenarios created here \n",
    "  round(COALESCE(csu.SLS_UNITS / csu.FLEXDAYS * 7, 0),2) as 1WOS_units,\n",
    "  round(COALESCE(arrv.arriv_units/91*7,0),2) as 1FWOS_units,\n",
    "  round(COALESCE(csu.SLS_UNITS / csu.FLEXDAYS * 14, 0),2) as 2WOS_units,\n",
    "  round(COALESCE(arrv.arriv_units/91*14,0),2) as 2FWOS_units,\n",
    "  round(COALESCE(csu.SLS_UNITS / csu.FLEXDAYS * 21, 0),2) as 3WOS_units,\n",
    "  round(COALESCE(arrv.arriv_units/91*21,0),2) as 3FWOS_units,\n",
    "  round(COALESCE(csu.SLS_UNITS / csu.FLEXDAYS * 28, 0),2) as 4WOS_units,\n",
    "  round(COALESCE(arrv.arriv_units/91*28,0),2) as 4FWOS_units,\n",
    "  csu.FLEXDAYS,\n",
    "  greatest(greatest(round(COALESCE(SLS_UNITS / FLEXDAYS * 28, 0),2),round(COALESCE(arrv.arriv_units/ 91 * 28, 0),2) ),\n",
    "           case \n",
    "             when spine.whse_cost * 25 <= 50\n",
    "                then 25\n",
    "             else floor(50/spine.whse_cost)\n",
    "           end\n",
    "          ) as PB_UNITS\n",
    "from item_spine spine\n",
    "left join CSU CSU\n",
    "  on spine.top_level_item=csu.top_level_item\n",
    "    and spine.STORE_NBR=csu.STORE_NBR\n",
    "\n",
    "-- OH_DATA\n",
    "left join \n",
    "  (select distinct \n",
    "    prod_id\n",
    "    ,STORE_NBR\n",
    "    ,OH_DLRS\n",
    "    ,OH_UNITS \n",
    "  from schema.day_store_ioh oha\n",
    "  join schema.dim_dates dt\n",
    "    on oha.per_cd=dt.per_cd\n",
    "  where dt.DAY_DATE = dateadd({0},-1)\n",
    "  ) oh\n",
    " on spine.prod_id=oh.prod_id\n",
    "  and spine.STORE_NBR=oh.STORE_NBR\n",
    "\n",
    "-- arrival_data\n",
    "left join \n",
    "  (select distinct\n",
    "    item_nbr,\n",
    "    str_nbr,\n",
    "    sum(qty) as arriv_units\n",
    "  from schema.plan_deliv\n",
    "  where arrivdate < to_date({0})+91\n",
    "    and run_Date = {0}\n",
    "  group by\n",
    "    item_nbr,\n",
    "    str_nbr\n",
    "  ) arrv\n",
    "  on spine.top_level_item=arrv.item_nbr\n",
    "    and spine.STORE_NBR=arrv.str_nbr\n",
    "\n",
    "order by \n",
    "  spine.top_level_item,\n",
    "  spine.STORE_NBR\n",
    "\"\"\"\n",
    "view_name = 'BASE_DATA'\n",
    "DATA = spark.sql(BASE_QUERY.format(OH_DT))\n",
    "DATA.createOrReplaceTempView(view_name)\n",
    "drop_query = \"\"\"delete from work_schema.cap_base\"\"\"\n",
    "insert_query = \"\"\"insert into work_schema.cap_base select * from BASE_DATA\"\"\"\n",
    "sqlContext.sql(drop_query)\n",
    "sqlContext.sql(insert_query)\n",
    "print(view_name,(DATA.count(), len(DATA.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detail History (Running 12 months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|  run_date|itm_count|\n",
      "+----------+---------+\n",
      "|1111-11-07|111111111|\n",
      "|1111-11-14|111111111|\n",
      "|1111-11-21|111111111|\n",
      "|1111-11-28|111111111|\n",
      "|1111-12-05|111111111|\n",
      "|1111-12-12|111111111|\n",
      "|1111-12-19|111111111|\n",
      "|1111-12-26|111111111|\n",
      "|1111-01-03|111111111|\n",
      "|1111-01-10|111111111|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "insert_query = f\"\"\"\n",
    "insert into work_schema.cap_details\n",
    "select distinct\n",
    "  to_date(todate) as run_date,\n",
    "  a.top_level_item,\n",
    "  a.STORE_NBR,\n",
    "  sub_days,\n",
    "  vnd_dlv_flg,\n",
    "  ITEMDIM4,\n",
    "  ord_mult,\n",
    "  --budget limit override occurs here\n",
    "  coalesce(b.cap_units,a.budget_dos) as budget_dos,\n",
    "  str_dos,              \n",
    "  ftr_dos as str_ftr_dos,\n",
    "  oh_units,\n",
    "  OH_DLRS,\n",
    "  SLS_UNITS,\n",
    "  CSU,\n",
    "  arriv_units,\n",
    "  arriv_cost,\n",
    "  1WOS_units,          \n",
    "  2WOS_units,         \n",
    "  3WOS_units,\n",
    "  4WOS_units,\n",
    "  1FWOS_units,         \n",
    "  2FWOS_units,       \n",
    "  3FWOS_units,\n",
    "  4FWOS_units,\n",
    "  FLEXDAYS,\n",
    "  PB_UNITS\n",
    "from work_schema.cap_base a\n",
    "left join work_schema.cap_mnl_ovrd b\n",
    "  on a.top_level_item=b.top_level_item\n",
    "  and a.STORE_NBR=b.STORE_NBR\n",
    "  and b.cap_units >= 0\n",
    "  and b.override_code = 'SBR'\n",
    "  and b.start_date <= {OH_DT}\n",
    "  and b.end_date >= {OH_DT} \n",
    "\"\"\"\n",
    "delete_query = \"\"\"\n",
    "delete from work_schema.cap_details\n",
    "where run_date = (select run_date from\n",
    "                  (select *,row_number() over (order by run_date desc) as rn\n",
    "                  from (select distinct run_date from work_schema.cap_details))\n",
    "                  where rn = 13)\"\"\"\n",
    "Review_Query = \"select run_date,count(*) from work_schema.cap_details group by run_date order by run_date\"\n",
    "sqlContext.sql(insert_query)\n",
    "sqlContext.sql(delete_query)\n",
    "DATA = spark.sql(Review_Query)\n",
    "DATA.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation (Encoding and Overwrites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morphset (123456789, 14)\n"
     ]
    }
   ],
   "source": [
    "view_name = 'morphset'\n",
    "encode_query = \"\"\"\n",
    " select distinct\n",
    "  fct.top_level_item,\n",
    "  fct.STORE_NBR,\n",
    " -------       block and limit coding       -------\n",
    " ---using subquery references here rather than storing all relevant dims in histories\n",
    "  case\n",
    "    when STORE_NBR in ({0})  \n",
    "      or STORE_NBR in (select distinct STORE_NBR from schema.dim_location where RGN_NBR in ({1}))\n",
    "      or top_level_item in ({2})\n",
    "      or top_level_item in (select distinct top_level_item from schema.dim_top_level_item where OB_IND = {35})\n",
    "      or top_level_item in (select distinct top_level_item from schema.dim_top_level_item where ITEM_DEPT in ({3}))\n",
    "      or top_level_item in (select distinct top_level_item from schema.dim_top_level_item where ITEMDIM2 in ({4}))\n",
    "        then 'A'\n",
    "    when STORE_NBR in ({5})  \n",
    "      or STORE_NBR in (select distinct STORE_NBR from schema.dim_location where RGN_NBR in ({6}))\n",
    "      or top_level_item in ({7})\n",
    "      or top_level_item in (select distinct top_level_item from schema.dim_top_level_item where OB_IND = {36})\n",
    "      or top_level_item in (select distinct top_level_item from schema.dim_top_level_item where ITEM_DEPT in ({8}))\n",
    "      or top_level_item in (select distinct top_level_item from schema.dim_top_level_item where ITEMDIM2 in ({9}))\n",
    "        then 'B'\n",
    "    when STORE_NBR in ({10})  \n",
    "      or STORE_NBR in (select distinct STORE_NBR from schema.dim_location where RGN_NBR in ({11}))\n",
    "      or top_level_item in ({12})\n",
    "      or top_level_item in (select distinct top_level_item from schema.dim_top_level_item where OB_IND = {37})\n",
    "      or top_level_item in (select distinct top_level_item from schema.dim_top_level_item where ITEM_DEPT in ({13}))\n",
    "      or top_level_item in (select distinct top_level_item from schema.dim_top_level_item where ITEMDIM2 in ({14}))\n",
    "        then 'C'\n",
    "    when STORE_NBR in ({15})  \n",
    "      or STORE_NBR in (select distinct STORE_NBR from schema.dim_location where RGN_NBR in ({16}))\n",
    "      or top_level_item in ({17})\n",
    "      or top_level_item in (select distinct top_level_item from schema.dim_top_level_item where OB_IND = {38})\n",
    "      or top_level_item in (select distinct top_level_item from schema.dim_top_level_item where ITEM_DEPT in ({18}))\n",
    "      or top_level_item in (select distinct top_level_item from schema.dim_top_level_item where ITEMDIM2 in ({19}))\n",
    "        then 'D'\n",
    "    else 'E'\n",
    "  end as block_code, \n",
    "  case\n",
    "    when STORE_NBR in ({20})  \n",
    "      or STORE_NBR in (select distinct STORE_NBR from schema.dim_location where RGN_NBR in ({21}))\n",
    "      or top_level_item in ({22})\n",
    "      or top_level_item in (select distinct top_level_item from schema.dim_top_level_item where OB_IND = {39})\n",
    "      or top_level_item in (select distinct top_level_item from schema.dim_top_level_item where ITEM_DEPT in ({23}))\n",
    "      or top_level_item in (select distinct top_level_item from schema.dim_top_level_item where ITEMDIM2 in ({24}))\n",
    "        then 'A'\n",
    "    when STORE_NBR in ({25})  \n",
    "      or STORE_NBR in (select distinct STORE_NBR from schema.dim_location where RGN_NBR in ({26}))\n",
    "      or top_level_item in ({27})\n",
    "      or top_level_item in (select distinct top_level_item from schema.dim_top_level_item where OB_IND = {40} )\n",
    "      or top_level_item in (select distinct top_level_item from schema.dim_top_level_item where ITEM_DEPT in ({28}))\n",
    "      or top_level_item in (select distinct top_level_item from schema.dim_top_level_item where ITEMDIM2 in ({29}))\n",
    "        then 'C'\n",
    "    when STORE_NBR in ({30})  \n",
    "      or STORE_NBR in (select distinct STORE_NBR from schema.dim_location where RGN_NBR in ({31}))\n",
    "      or top_level_item in ({31})\n",
    "      or top_level_item in (select distinct top_level_item from schema.dim_top_level_item where OB_IND = {41} )\n",
    "      or top_level_item in (select distinct top_level_item from schema.dim_top_level_item where ITEM_DEPT in ({33}))\n",
    "      or top_level_item in (select distinct top_level_item from schema.dim_top_level_item where ITEMDIM2 in ({34}))\n",
    "        then 'D'\n",
    "    else 'B'\n",
    "  end as cap_weeks_code,\n",
    " ----------------------------- Block Threshold Binaries ----------------------------------------\n",
    "  case\n",
    "    when fct.budget_dos <= least(fct.str_dos,decode(fct.str_ftr_dos,0,fct.str_dos,fct.str_ftr_dos))\n",
    "      then 1 \n",
    "    else 0\n",
    "  end as 0_block,\n",
    "  case\n",
    "    when fct.budget_dos - 7 <= least(fct.str_dos,decode(fct.str_ftr_dos,0,fct.str_dos,fct.str_ftr_dos))\n",
    "      then 1 \n",
    "    else 0\n",
    "  end as 1_block,\n",
    "  case\n",
    "    when fct.budget_dos - 14 <= least(fct.str_dos,decode(fct.str_ftr_dos,0,fct.str_dos,fct.str_ftr_dos))\n",
    "      then 1 \n",
    "    else 0\n",
    "  end as 2_block,\n",
    "  case\n",
    "    when fct.budget_dos - 21 <= least(fct.str_dos,decode(fct.str_ftr_dos,0,fct.str_dos,fct.str_ftr_dos))\n",
    "      then 1 \n",
    "    else 0\n",
    "  end as 3_block,\n",
    "  case\n",
    "    when fct.budget_dos - 28 <= least(fct.str_dos,decode(fct.str_ftr_dos,0,fct.str_dos,fct.str_ftr_dos))\n",
    "      then 1 \n",
    "    else 0\n",
    "  end as 4_block,\n",
    " ----------------------------- CAP Scenarios (Selecting greatest of future and historical days of supply and Order Mult Rounding)  ----------------------------------------\n",
    "  greatest(\n",
    "    case \n",
    "      when fct.1WOS_units = 0 \n",
    "        and fct.oh_units > 0\n",
    "          then 0\n",
    "      when fct.1WOS_units < fct.ord_mult\n",
    "        then fct.ord_mult\n",
    "      else ceil(fct.1WOS_units/fct.ord_mult)*fct.ord_mult\n",
    "    end, \n",
    "    case \n",
    "      when fct.1FWOS_units = 0 \n",
    "        and fct.oh_units > 0\n",
    "          then 0\n",
    "      when fct.1FWOS_units < fct.ord_mult\n",
    "          then fct.ord_mult\n",
    "      else ceil(fct.1FWOS_units/fct.ord_mult)*fct.ord_mult\n",
    "    end ) as 1WOS_CAP,\n",
    "\n",
    "  greatest(\n",
    "    case \n",
    "      when fct.2WOS_units = 0 \n",
    "        and fct.oh_units > 0\n",
    "          then 0\n",
    "      when fct.2WOS_units < fct.ord_mult\n",
    "          then fct.ord_mult\n",
    "      else ceil(fct.2WOS_units/fct.ord_mult)*fct.ord_mult\n",
    "    end, \n",
    "    case \n",
    "      when fct.2FWOS_units = 0 \n",
    "        and fct.oh_units > 0\n",
    "          then 0\n",
    "      when fct.2FWOS_units < fct.ord_mult\n",
    "          then fct.ord_mult\n",
    "      else ceil(fct.2FWOS_units/fct.ord_mult)*fct.ord_mult\n",
    "    end) as 2WOS_CAP,\n",
    "\n",
    "  greatest(\n",
    "    case \n",
    "      when fct.3WOS_units = 0 \n",
    "        and fct.oh_units > 0\n",
    "          then 0\n",
    "      when fct.3WOS_units < fct.ord_mult\n",
    "        then fct.ord_mult\n",
    "      else ceil(fct.3WOS_units/fct.ord_mult)*fct.ord_mult\n",
    "    end, \n",
    "    case \n",
    "      when fct.3FWOS_units = 0 \n",
    "        and fct.oh_units > 0\n",
    "          then 0\n",
    "      when fct.3FWOS_units < fct.ord_mult\n",
    "        then fct.ord_mult\n",
    "      else ceil(fct.3FWOS_units/fct.ord_mult)*fct.ord_mult\n",
    "    end) as 3WOS_CAP,\n",
    "\n",
    "  greatest(\n",
    "    case \n",
    "      when fct.4WOS_units = 0 \n",
    "        and fct.oh_units > 0\n",
    "          then 0\n",
    "      when fct.4WOS_units < fct.ord_mult\n",
    "        then fct.ord_mult\n",
    "      else ceil(fct.4WOS_units/fct.ord_mult)*fct.ord_mult\n",
    "    end, \n",
    "    case \n",
    "      when fct.4FWOS_units = 0 \n",
    "        and fct.oh_units > 0\n",
    "          then 0\n",
    "      when fct.4FWOS_units < fct.ord_mult\n",
    "        then fct.ord_mult\n",
    "      else ceil(fct.4FWOS_units/fct.ord_mult)*fct.ord_mult\n",
    "    end) as 4WOS_CAP,\n",
    "\n",
    "  run_date,\n",
    "  PB_UNITS\n",
    " from work_schema.cap_details fct\n",
    " \n",
    " where run_date = (select max(run_date) from work_schema.cap_details)\n",
    " \n",
    " order by\n",
    "  top_level_item,\n",
    "  STORE_NBR \"\"\"\n",
    "DATA = spark.sql(encode_query.format(one_blk['STR'],        #0   \n",
    "                                    one_blk['RGN'],         #1   \n",
    "                                    one_blk['TLI'],         #2   \n",
    "                                    one_blk['DEPT'],        #3   \n",
    "                                    one_blk['ITEMDIM2'],    #4  \n",
    "                                    two_blk['STR'],         #5  \n",
    "                                    two_blk['RGN'],         #6  \n",
    "                                    two_blk['TLI'],         #7\n",
    "                                    two_blk['DEPT'],        #8                                       \n",
    "                                    two_blk['ITEMDIM2'],    #9\n",
    "                                    three_blk['STR'],       #10  \n",
    "                                    three_blk['RGN'],       #11  \n",
    "                                    three_blk['TLI'],       #12\n",
    "                                    three_blk['DEPT'],      #13                                      \n",
    "                                    three_blk['ITEMDIM2'],  #14   \n",
    "                                    four_blk['STR'],        #15\n",
    "                                    four_blk['RGN'],        #16\n",
    "                                    four_blk['TLI'],        #17\n",
    "                                    four_blk['DEPT'],       #18                               \n",
    "                                    four_blk['ITEMDIM2'],   #19  \n",
    "                                    one_cap['STR'],         #20\n",
    "                                    one_cap['RGN'],         #21\n",
    "                                    one_cap['TLI'],         #22\n",
    "                                    one_cap['DEPT'],        #23                            \n",
    "                                    one_cap['ITEMDIM2'],    #24  \n",
    "                                    three_cap['STR'],       #25\n",
    "                                    three_cap['RGN'],       #26\n",
    "                                    three_cap['TLI'],       #27\n",
    "                                    three_cap['DEPT'],      #28                             \n",
    "                                    three_cap['ITEMDIM2'],  #29   \n",
    "                                    four_cap['STR'],        #30\n",
    "                                    four_cap['RGN'],        #31\n",
    "                                    four_cap['TLI'],        #32\n",
    "                                    four_cap['DEPT'],       #33                            \n",
    "                                    four_cap['ITEMDIM2'],   #34\n",
    "                                    OBL['1WOS'],               #35   \n",
    "                                    OBL['3WOS'],              #36   \n",
    "                                    OBL['4WOS'],              #37   \n",
    "                                    OBB['2WOS'],              #38   \n",
    "                                    OBB['1WOS'],               #39   \n",
    "                                    OBB['3WOS'],              #40   \n",
    "                                    OBB['4WOS']               #41\n",
    "                                  ))\n",
    "print(view_name,(DATA.count(), len(DATA.columns)))\n",
    "DATA.createOrReplaceTempView(view_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special Rule Overrides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morphset (123456789, 15)\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\" \n",
    "select distinct \n",
    "  a.run_date,\n",
    "  a.top_level_item,\n",
    "  a.STORE_NBR,\n",
    "  case\n",
    "    when b.override_code = 'SBR'\n",
    "    and b.cap_units < 0 \n",
    "      then b.reason_code\n",
    "    else coalesce(c.reason_code,a.block_code)\n",
    "  end as block_code,\n",
    "  case \n",
    "    when b.override_code in ('MCR','SDR','SCR') \n",
    "      then b.reason_code\n",
    "    else coalesce(c.reason_code,a.cap_weeks_code) \n",
    "  end as cap_weeks_code,\n",
    "  1WOS_CAP,\n",
    "  2WOS_CAP,\n",
    "  3WOS_CAP,\n",
    "  4WOS_CAP,\n",
    "  coalesce(c.cap_units, b.cap_units) as spec_rl_cap_units,\n",
    "  case\n",
    "    when b.override_code = 'SBR'\n",
    "    and b.cap_units = 0 \n",
    "      then 1\n",
    "    else 0_block \n",
    "  end as 0_block,\n",
    "  case\n",
    "    when b.override_code = 'SBR'\n",
    "    and b.cap_units = 0 \n",
    "      then 1\n",
    "    else 1_block \n",
    "  end as 1_block,\n",
    "  case\n",
    "    when b.override_code = 'SBR'\n",
    "    and b.cap_units = 0 \n",
    "      then 1\n",
    "    else 2_block \n",
    "  end as 2_block,\n",
    "  case\n",
    "    when b.override_code = 'SBR'\n",
    "    and b.cap_units = 0 \n",
    "      then 1\n",
    "    else 3_block \n",
    "  end as 3_block,\n",
    "  case\n",
    "    when b.override_code = 'SBR'\n",
    "    and b.cap_units = 0 \n",
    "      then 1\n",
    "    else 4_block \n",
    "  end as 4_block\n",
    "from {0} a\n",
    "left join -- non exemption overwrites\n",
    "  (select * from work_schema.cap_mnl_ovrd \n",
    "  where start_date <= {1} and end_date >= {1} \n",
    "  and override_code <> 'SER') b\n",
    "on a.top_level_item = b.top_level_item\n",
    "and a.STORE_NBR = b.STORE_NBR\n",
    "left join  -- exemption overwrites\n",
    "  (select * from work_schema.cap_mnl_ovrd \n",
    "  where start_date <= {1} and end_date >= {1} \n",
    "  and override_code = 'SER') c\n",
    "on a.top_level_item = c.top_level_item\n",
    "and a.STORE_NBR = c.STORE_NBR \"\"\"\n",
    "DATA = spark.sql(query.format(view_name, OH_DT))\n",
    "DATA.createOrReplaceTempView(view_name)\n",
    "print(view_name,(DATA.count(), len(DATA.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEGACY-NEW ITEM CODE RELATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morphset (133456789, 14)\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\" \n",
    "select distinct\n",
    "  a.run_date,\n",
    "  b.upld_str_cd,\n",
    "  a.STORE_NBR,\n",
    "  coalesce(c.upld_item_code,d.upld_item_code) as upld_item_code,\n",
    "  a.top_level_item,\n",
    "  a.reason_code,\n",
    "  a.block_code,\n",
    "  a.cap_weeks_code,\n",
    "  a.dsd_limit_units,\n",
    "  a.1WOS_CAP,\n",
    "  a.2WOS_CAP,\n",
    "  a.3WOS_CAP,\n",
    "  a.4WOS_CAP,\n",
    "  a.spec_rl_cap_units,\n",
    "  a.0_block,\n",
    "  a.1_block,\n",
    "  a.2_block,\n",
    "  a.3_block,\n",
    "  a.4_block\n",
    "from {0} a\n",
    "join schema.loc_xref b\n",
    "  on a.STORE_NBR = b.loc_num\n",
    "left join \n",
    "    (select distinct \n",
    "      TL_ITEM, \n",
    "      upld_item_code \n",
    "    from schema.item_xref\n",
    "    ) c\n",
    "  on a.top_level_item = c.TL_ITEM\n",
    "  left join \n",
    "    (select distinct \n",
    "      midlevelitem, \n",
    "      upld_item_code \n",
    "    from schema.item_xref \n",
    "    ) d\n",
    "on substr(a.top_level_item,7,6) = d.midlevelitem\n",
    "   \"\"\"\n",
    "DATA = spark.sql(query.format(view_name))\n",
    "print(view_name,(DATA.count(), len(DATA.columns)))\n",
    "DATA.createOrReplaceTempView(view_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morphset (133456789, 15)\n"
     ]
    }
   ],
   "source": [
    "#standardize setup duplicate errors in source data on max\n",
    "query = \"\"\" \n",
    "select distinct\n",
    "  a.run_date,\n",
    "  a.upld_str_cd,\n",
    "  a.STORE_NBR,\n",
    "  a.upld_item_code,\n",
    "  a.top_level_item,\n",
    "  b.min_block_code as block_code,\n",
    "  b.max_limit_code as cap_weeks_code,\n",
    "  b.1WOS_CAP,\n",
    "  b.2WOS_CAP,\n",
    "  b.3WOS_CAP,\n",
    "  b.4WOS_CAP,\n",
    "  b.spec_rl_cap_units,\n",
    "  b.0_block,\n",
    "  b.1_block,\n",
    "  b.2_block,\n",
    "  b.3_block,\n",
    "  b.4_block\n",
    "from {0} a\n",
    "join\n",
    "  (select distinct \n",
    "    upld_str_cd,\n",
    "    upld_item_code,\n",
    "    decode(min(decode(block_code,\n",
    "                      'M'   , 1,\n",
    "                      'E'   , 2,\n",
    "                      'A'   , 3,\n",
    "                      'B'   , 4,\n",
    "                      'C'   , 5,\n",
    "                      'D'   , 6 )),\n",
    "            2 , 'M',\n",
    "            2 , 'E',\n",
    "            3 , 'A',\n",
    "            4 , 'B',\n",
    "            5 , 'C',\n",
    "            6 , 'D') \n",
    "    as min_block_code,\n",
    "    decode(max(decode(cap_weeks_code,\n",
    "                      'A'   , 1,\n",
    "                      'B'   , 2,\n",
    "                      'C'   , 4,\n",
    "                      'D'   , 5,\n",
    "                      'M'   , 6 )),\n",
    "          1 , 'A',\n",
    "          2 , 'B',\n",
    "          4 , 'C',\n",
    "          5 , 'D',\n",
    "          6 , 'M') \n",
    "    as max_limit_code,\n",
    "    max(1WOS_CAP) as 1WOS_CAP,\n",
    "    max(2WOS_CAP) as 2WOS_CAP,\n",
    "    max(3WOS_CAP) as 3WOS_CAP,\n",
    "    max(4WOS_CAP) as 4WOS_CAP,\n",
    "    max(spec_rl_cap_units) as spec_rl_cap_units,\n",
    "    max(0_block) as 0_block,\n",
    "    max(1_block) as 1_block,\n",
    "    max(2_block) as 2_block,\n",
    "    max(3_block) as 3_block,\n",
    "    max(4_block) as 4_block\n",
    "  from {0}\n",
    "  group by\n",
    "    upld_str_cd,\n",
    "    upld_item_code) b\n",
    "on a.upld_str_cd = b.upld_str_cd\n",
    "  and a.upld_item_code = b.upld_item_code \"\"\"\n",
    "DATA = spark.sql(query.format(view_name))\n",
    "print(view_name,(DATA.count(), len(DATA.columns)))\n",
    "DATA.createOrReplaceTempView(view_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision History, TW/LW Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morphset (123456789, 9)\n",
      "+----------+------+---------+\n",
      "|  run_date|action| it_count|\n",
      "+----------+------+---------+\n",
      "|1111-01-10|Delete|   111111|\n",
      "|1111-01-10|   DUP|111111111|\n",
      "|1111-01-10| Insrt|   111111|\n",
      "|1111-01-10|  Updt| 11111111|\n",
      "+----------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cap_query = \"\"\"\n",
    "  select distinct\n",
    "    a.run_date,\n",
    "    a.top_level_item,\n",
    "    a.upld_item_code,\n",
    "    a.STORE_NBR,\n",
    "    a.upld_str_cd,\n",
    "    case \n",
    "      when coalesce(hist.delta_desc,0) = 0\n",
    "        then 'Insrt'\n",
    "      when cast(hist.cap_units as int) <> cast(a.cap_units as int)\n",
    "        then 'Updt'\n",
    "      else 'DUP'\n",
    "    end as delta_desc,\n",
    "    a.lgc_cd,\n",
    "    a.cap_units\n",
    "  from \n",
    "  (select distinct \n",
    "    run_date,\n",
    "    upld_item_code,\n",
    "    top_level_item,\n",
    "    STORE_NBR,\n",
    "    upld_str_cd,\n",
    "    block_code||'-'||cap_weeks_code as lgc_cd,\n",
    "    case \n",
    "      when decode(block_code,\n",
    "                  'A'   , 1_block,\n",
    "                  'B'   , 2_block,\n",
    "                  'C'   , 3_block,\n",
    "                  'D'   , 4_block,\n",
    "                  'E'   , 0_block,\n",
    "                  'M'   , 0) = 1\n",
    "        then 0\n",
    "      else decode(cap_weeks_code,\n",
    "                  'A', 1WOS_CAP,\n",
    "                  'B', 2WOS_CAP,\n",
    "                  'C', 3WOS_CAP,\n",
    "                  'D', 4WOS_CAP,\n",
    "                  'M', spec_rl_cap_units)\n",
    "    end as cap_units\n",
    "  from {0}\n",
    "    ) a\n",
    "  left join \n",
    "    (select distinct\n",
    "      upld_item_code,\n",
    "      upld_str_cd,\n",
    "      delta_desc,\n",
    "      cap_units\n",
    "    from work_schema.cap_hist\n",
    "      where run_date = (select max(run_date) from work_schema.cap_hist)\n",
    "    ) hist\n",
    "    on a.upld_item_code = hist.upld_item_code\n",
    "    and a.upld_str_cd = hist.upld_str_cd \"\"\"\n",
    "Review_Query = \"\"\"\n",
    "  select distinct \n",
    "    delta_desc,\n",
    "    count(*)\n",
    "  from limits\n",
    "  group by delta_desc\"\"\"\n",
    "Review_Query2 = \"\"\"\n",
    "  select distinct\n",
    "  run_date,\n",
    "  delta_desc,\n",
    "  count(*)\n",
    "  from work_schema.cap_hist\n",
    "    where run_date = (select max(run_date) from work_schema.cap_hist)\n",
    "  group by\n",
    "    delta_desc,\n",
    "    run_date\n",
    "  order by\n",
    "  run_date,\n",
    "  delta_desc\"\"\"\n",
    "DATA = spark.sql(cap_query.format(view_name))\n",
    "print(view_name,(DATA.count(), len(DATA.columns)))\n",
    "DATA.createOrReplaceTempView('caps')\n",
    "dt = query_to_variable(\"select distinct run_Date from caps\")\n",
    "insert_query = \"\"\"\n",
    "insert into work_schema.cap_hist\n",
    "select \n",
    "    top_level_item,\n",
    "    upld_item_code,\n",
    "    STORE_NBR,\n",
    "    upld_str_cd,\n",
    "    lgc_cd,\n",
    "    cap_units,\n",
    "    delta_desc,\n",
    "    run_date\n",
    "from limits\n",
    "union\n",
    "Select\n",
    "    a.top_level_item,\n",
    "    a.upld_item_code,\n",
    "    a.STORE_NBR,\n",
    "    a.upld_str_cd,\n",
    "    a.lgc_cd,\n",
    "    a.cap_units,\n",
    "    'Delete' as delta_desc,\n",
    "    '{0}' as run_date\n",
    "from work_schema.cap_hist a\n",
    "--identify all prior records not in current week --\n",
    "anti join caps b\n",
    "    on a.upld_item_code = b.upld_item_code\n",
    "    and a.upld_str_cd = b.upld_str_cd\n",
    "where a.run_date = (select max(run_date) from work_schema.cap_hist)\n",
    "and a.delta_desc <> 'Delete'\n",
    "\"\"\".format(dt)\n",
    "sqlContext.sql(insert_query)\n",
    "DATA = spark.sql(Review_Query2)\n",
    "DATA.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_query = \"\"\" delete from work_schema.upload_table \"\"\"\n",
    "sqlContext.sql(delete_query)\n",
    "insert_query = \"\"\"\n",
    "  insert into work_schema.upload_table\n",
    "  select distinct\n",
    "    upld_str_cd,\n",
    "    upld_item_code,\n",
    "    cap_units,\n",
    "    delta_desc\n",
    "  from work_schema.cap_hist\n",
    "    where run_date = (select max(run_date) from work_schema.cap_hist)\n",
    "    and delta_desc <> 'DUP'\n",
    "  \"\"\"\n",
    "DATA = spark.sql(insert_query)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
